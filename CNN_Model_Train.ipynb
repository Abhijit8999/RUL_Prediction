{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Model_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkVW9VD4f7KNUyDsu407kY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijit8999/RUL_Prediction/blob/main/CNN_Model_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoQEwP2BdM81"
      },
      "source": [
        "#Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiQku1Mccpoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2626f33-1674-4e3c-e6d8-6350c9e79fc8"
      },
      "source": [
        "'''\n",
        "Mounting => Before your computer can use any kind of storage device (such as hard drive, Google drive), you or your operating system must make it\n",
        "accessible through the computer’s file system. This process is called mounting. You can only access files on mounted media.\n",
        "In Computers, to mount is to make a group of files in a file system structure accessible to a user or user group. In some usages, it means to make a\n",
        "device physically accessible. Mounting a file system (Google drive) attaches that Google drive to a directory (mount  point) and makes it available to the\n",
        "system. In simple words, with mounting a Google drive, user and operating system can access to all the files present in the Google drive. A mounted disk \n",
        "(a mounted drive) is available to the operating system as a file system, for reading, writing, or both.\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbB7mbY5d22W"
      },
      "source": [
        "#Importing the Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BmCjSZWdbKR"
      },
      "source": [
        "'''\n",
        "Python library = python library is the collection of modules(= python files) and this python library is the reusable(=able to use again) chunk(= part, section\n",
        ", block) of code we want to include in our python programs or projects to make the implementation easier and faster.\n",
        "\n",
        "os module in python provides functions for interacting with the operating system. os module in Python provides functions for creating and removing a \n",
        "directory(folder), fetching its contents, os module used for changing and identifying the current directory, etc. Basically os module allows source code\n",
        "to communicate (interact) with operating system.\n",
        "\n",
        "Pickle in Python is primarily used in serializing and deserializing a Python object structure. In other words, it's the process of converting a python\n",
        "object into a byte stream to store it in a file/database. Basically pickle library is used to dump (store) all the files of a directory (folder) into \n",
        "single combined file (pickle(.pkz) file) for easy fetching and fast retrieval of data. pickle.dump() is used to create a pickle file, it is used to dump\n",
        "(store) data in a pickle file. pickle.load() is used to load(= start, activate) pickle file\n",
        "\n",
        "Torch (or PyTorch) is an open-source library for machine learning and deep learning. Torch library provides a wide range of algorithms for machine learning and \n",
        "deep learning. The core package of Torch is “torch”, torch package provides a flexible (= adaptable, adjustable, alterable, changeable) N-dimensional array or \n",
        "Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. The Tensor also supports \n",
        "mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix-vector \n",
        "multiplication, matrix-matrix multiplication, matrix-vector product and matrix product. The torch package also simplifies object oriented programming and \n",
        "serialization by providing various convenience functions which are used throughout its packages \n",
        "\n",
        "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled (=separated) from our model training code \n",
        "for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow us to use \n",
        "pre-loaded datasets as well as our own data. DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
        "\n",
        "torch.utils.data.Dataset is an abstract class representing a dataset. custom dataset (torch dataset) should inherit Dataset and apply the following methods:\n",
        "(1)__init__ , (2)__len__ , (3)__getitem__ , and (4) an optional argument transform. Dataset stores the samples and their corresponding labels\n",
        "\n",
        "torch.utils.data.DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
        "\n",
        "numpy module allows us to work with numerical data. numpy provides an object called numpy array. numpy supports large multi-dimensional arrays & matrices. \n",
        "Basically numpy is a python library used for working with arrays. numpy used for arithmetic operations, statistical operations, bitwise operations, copying \n",
        "and viewing arrays, stacking, matrix operations, linear algebra, mathematical operations, searching, sorting, and counting.\n",
        " \n",
        "as keyword is used as alias (AKA, also called as)\n",
        "'''\n",
        "import os\n",
        "import pickle as pkl\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Ud14N3lDD_"
      },
      "source": [
        "#PHM IEEE Dataset\n",
        "Helper Function = A helper function is a function that performs part of the computation (= operation, calculation, estimation, guess) of another function. Helper functions are used to make our programs (= codes) easier to read by giving names to computations. Helper functions also let you reuse computations, just as with functions in general. A helper function is a function we write because we need that particular functionality (= purpose, operation) of a function in multiple places in a code, and because it makes the code more readable. Instead of defining a particular functionality many times, insert(=put, embed) the functionality which we required many times in a helper function, so that we can use that particular functionality as many times we required without defining again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "716LRg5ql0ua"
      },
      "source": [
        "Loading data from pickle(.pkz) file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oKvFrIceNwY"
      },
      "source": [
        "'''\n",
        "#Load Data from Pickle File\n",
        "\n",
        "def keyword is used to define (= create) a function or method in python\n",
        "\n",
        "using def keyword we defined (= created) a function load_data_from_pfile and passed an argument file_path. Argument is the value to be passed in a function. \n",
        "Here file_path is the location of all pickle files. i.e. file_path = /content/drive/MyDrive/Colab Notebooks \n",
        "\n",
        "with keyword => automatically releases memory after allocation. Whenever we open the file with open() function, it allocates some resources and memory to the \n",
        "file. And we should use close() function to release or delete that memory from the file otherwise errors will come. Sometimes we forget to close() the file and \n",
        "we couldn’t find that we didn’t closed the file, so even the whole code is correct we might get errors and we may not be able to correct it. So it’s better to \n",
        "use “with” keyword along with open() function as “with” keyword automatically releases or deletes memory after process completion\n",
        "\n",
        "The open() function opens a file in text format by default. To open a file in binary format, add 'b' to the mode parameter. Hence the \"rb\" mode opens the file \n",
        "in binary format for reading, while the \"wb\" mode opens the file in binary format for writing. (Note: there are 2 basic mode parameters (r = read mode,w = write \n",
        "mode)). Unlike text files, binary files are not human-readable\n",
        "\n",
        "as = The as keyword is used to create an alias (= aka, also known as, also called). In the code, we created an alias pfile when opening the file_path, and now \n",
        "we can refer to the file_path (or we can access the file_path) by using pfile instead of file_path.\n",
        "\n",
        "Basically The pickle(.pkz) file is created using Python pickle and the dump() method and is loaded (=started, activated) using Python pickle and the load() \n",
        "method. we imported(=send) pickle module as pkl in the code. Therefore pkl.dump() is used to create pickle(.pkz) file and pkl.load() is used to load(=start, \n",
        "activate) pickle file. Here, pfile is the pickle file. Pkl.load(pfile) loads (= starts, activates) the pfile in rb (read binary) mode. We stored the loaded \n",
        "pfile in sample_data variable.\n",
        "\n",
        "return keyword = the return keyword is used to exit (= come out from) a function and return a value. Return sample_data => returns value of sample_data and \n",
        "                 exits a function.\n",
        "\n",
        "'''\n",
        "def load_data_from_pfile(file_path):\n",
        "    with open(file_path, 'rb') as pfile:\n",
        "        sample_data = pkl.load(pfile)\n",
        "    return sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6imAfWRnSMW"
      },
      "source": [
        "torch Dataset class\n",
        "\n",
        "(Converting numpy array (nd array) into tensor)\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "torch.utils.data is a data loading (=activation) class of Torch (or PyTorch)\n",
        "\n",
        "torch.utils.data.Dataset is an abstract class representing(= considered as) a dataset. custom dataset (= if the dataset can be modified (=changed) according to our (user’s) requirements, then dataset is considered as custom dataset, for example we are modifying torch dataset in the code, so torch dataset is the custom dataset) should inherit (= acquire, obtain, derive, get, receive) Dataset and apply (= use) the following methods:\n",
        "\n",
        "(1) `__len__` so that len(dataset) returns the size of the dataset\n",
        "\n",
        "(2) `__getitem__` to support the indexing such that dataset[i] can be used to get ith sample (data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcmVQ2eOmLN9"
      },
      "source": [
        "'''\n",
        "Python classes and objects => https://www.w3schools.com/python/python_classes.asp\n",
        "A Class is like an object constructor, or a blueprint for creating objects. and objects of the class is used as references to access (= read, get, obtain,\n",
        "examine, acquire) the class properties (properties = variables, arrays, lists, etc.)\n",
        "\n",
        "Python Inheritance => https://www.w3schools.com/python/python_inheritance.asp\n",
        "Inheritance allows us to define a class that inherits (= acquires, obtains, gets, recieves) all the methods and properties from another class.\n",
        "\n",
        "Created PHMDataset() class using class keyword. we send Dataset as the parameter in the class. To create a class that inherits the functionality (=methods and \n",
        "properties) from another class, send the parent class as a parameter when creating the child class. Here, parameter Dataset is the parent class and PHMDataset() \n",
        "is the child class. Parent class is a class from which a class inherits, parent class also called as base class or super class. Child class is a class which \n",
        "inherits from another class (parent class), child class also called as derived class or sub class. class PHMDatset(Dataset) => this means PHMDataset() class \n",
        "inherits the functionality (= methods and propperties) from Dataset class\n",
        "\n",
        "Using def keyword, we defined (= created) __init__ function. __init__ function is used to assign values to class object properties or other operations that are \n",
        "necessary to do when the object is being created. The __init__() function is called automatically every time the class is being used to create a new object. We \n",
        "added __init__() function to the child class, therefore the child class will no longer inherit the parent's __init__() function. The child's __init__() \n",
        "function overrides (= removes) the inheritance of the parent's __init__() function. \n",
        "The self parameter is a reference to the current (= present, ongoing) instance (= object) of the class, and is used to access (= read, obtain, examine, acquire) \n",
        "variables that belongs to the class. (reference in python = a python program accesses (= reads, obtains, gets) data values through (= via) references. A \n",
        "reference is a name that refers to the specific (= certain, particular) location in memory of a value (object))\n",
        "created pfiles list and passed as a parameter in __init__ function and assigned an empty list to the pfiles. __init__ function helps in assigning the values.\n",
        "\n",
        "self.data = {'x': [], 'y': []} => here, self acts as a class (PHMDataset() class) object and data is the dictionary with keys x and y present in PHMDataset() \n",
        "class. x key referred to an empty list. y key also referred to an empty list. since self is the object of PHMDataset() class, we can access data dictionary of\n",
        "PHMDataset class using self parameter i.e. self.data\n",
        "Total 6 different bearing training dataset pickle files are there, therefore used for loop to access one training pickle file at a time from 6 different pickle\n",
        "files\n",
        "underscore can be used as variable name in python. created _data variable, _data is the single leading underscore variable. created _data variable and assigned\n",
        "(= stored) load_data_from_pfile() function which we created previously and passed pfile as a parameter inside the function. this load_data_from_pfile() function\n",
        "returns the data of pickle file as an output.\n",
        "The append() method in python adds a single item to the existing list. It doesn’t return a new list of items but will modify the original list by adding the \n",
        "item to the end of the list. After executing the append method on the list the size of the list increases by one. \n",
        "self.data['x'].append(_data['x']) => _data['x'] contains the list of horiz accel feature values and vert accel feature values, using append method added \n",
        "_data['x'] values to the data dictionary with x key (i.e. data['x']). In data dictionary (data['x']), x key represents an empty list therefore values of _data[x] \n",
        "added at the end of x empty list. self parameter is used to access.\n",
        "Therefore, self.data['x'] contains the list of horiz accel feature values and vert accel feature values of pfile (single pickle file)\n",
        "self.data['y'].append(_data['y']) => _data['y'] contains the list of failure probability values, using append method added _data['y'] values to the data \n",
        "dictionary with y key (i.e. data['y']). In data dictionary (data['y']), y key represents an empty list therefore values of _data[y] added at the end of y empty \n",
        "list. self parameter is used to access.\n",
        "Therefore, self.data['y'] contains the list of failure probability values of pfile (single pickle file)\n",
        "\n",
        "numpy.concatenate() => https://www.tutorialspoint.com/numpy/numpy_concatenate.htm\n",
        "Concatenation refers to joining. The concatenate() function is used to join two or more arrays of the same shape along a specified axis. The function takes the \n",
        "following parameters. general syntax => numpy.concatenate((a1, a2, ...), axis), here a1, a2, ... => represents Sequence (= order, arrangement) of arrays of the \n",
        "same type and axis => represents Axis along which arrays have to be joined. Default is 0.\n",
        "self.data['x'] = np.concatenate(self.data['x']) => joining list of horiz accel feature values and vert accel feature values of all 6 different bearing\n",
        "training pickle files as one. Therefore, self.data['x'] now contains the list of horiz accel feature values and vert accel feature values of all pfiles (6 \n",
        "different bearing training pickle files)\n",
        "self.data['y'] = np.concatenate(self.data['y']) => joining list of failure probability values of all 6 different bearing training pickle files as one. Therefore\n",
        "self.data['y'] now contains the list of failure probability values of all pfiles (6 different bearing training pickle files)\n",
        "np.newaxis is used to add the new axis (= new dimension). Hence, self.data['y'] values are stored in new dimension (= new axis)\n",
        "\n",
        "def keyword is used to create a function. using def keyword defined (= created) __len__() function and passed self parameter as an argument. __len__() function\n",
        "returns the size of the dataset. we passed self parameter in __len__() function, hence we can access all self parameter or self object properties inside the\n",
        "__len__() function.\n",
        "return self.data['x'].shape[0] => here, __len__() function returns the shape of data['x'] in 0th dimension. shape in python returns the no. of elements in each\n",
        "dimension. shape returns the output in tuple format. shape[0] => returns the no. of elements in 0th dimension. in data['x'] in 0th dimension(data['x'].shape[0]) \n",
        "=> total no. of training pickle data files are present i.e. total no. of horiz accel feature elements and vert accel feature elements are present\n",
        "\n",
        "def keyword is used to create a function. using def keyword defined (= created) __getitem__() function and passed self parameter and i as argmuments. arguments\n",
        "are the values that can be passed inside the function. arguments should be passed inside the parenthesis of function name. __getitem__() is used to support the \n",
        "indexing such that dataset[i] can be used to get ith sample (ith data). we passed self parameter in __getitem__() function, hence we can access all self \n",
        "parameter or self object properties inside the __getitem__() function. i is the index\n",
        "the function torch.from_numpy() => converts a numpy array into a tensor \n",
        "numpy array => https://www.javatpoint.com/numpy-array, \n",
        "tensor => https://www.javatpoint.com/pytorch-tensors#:~:text=A%20tensor%20is%20an%20n,computation%20with%20strong%20GPU%20acceleration. \n",
        "tensor = torch.from_numpy(numpy.array) => general syntax to Create a Tensor from numpy array\n",
        "i is used to access the ith data in tensor\n",
        "created sample dictionary with x and y keys. x key contains tensor of self.data['x'][i] i.e. contains the tensor of all horiz accel feature values and vert\n",
        "accel feature values. y key contains tensor of self.data['y'][i] i.e. contains the tensor of all failure probability values\n",
        "return sample => __getitem__() function return the sample dictionary as an output and exits the function\n",
        "\n",
        "'''\n",
        "class PHMDataset(Dataset):\n",
        "    '''\n",
        "    PHM IEEE 2012 Data Challenge Training data set (6 different Mechanical Bearings data)\n",
        "    '''\n",
        "    def __init__(self, pfiles=[]):\n",
        "        self.data = {'x': [], 'y': []}\n",
        "        for pfile in pfiles:\n",
        "            _data = load_data_from_pfile(pfile)\n",
        "            self.data['x'].append(_data['x'])\n",
        "            self.data['y'].append(_data['y'])\n",
        "        self.data['x'] = np.concatenate(self.data['x'])\n",
        "        self.data['y'] = np.concatenate(self.data['y'])[:,np.newaxis]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data['x'].shape[0]\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        sample = {'x': torch.from_numpy(self.data['x'][i]), 'y': torch.from_numpy(self.data['y'][i])}\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOxW1jH63n2H"
      },
      "source": [
        "storing 6 different bearings train data pickle files in a single list (train_pfiles) and 6 different bearings validation data pickle files in a single list (val_pfiles)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6YSoag0oLAO"
      },
      "source": [
        "'''\n",
        "created pkzfiles_path variable, and assigned(= allocate, allot, set) the path where the pickle(.pkz) files were saved in the pc or in the google drive.\n",
        "\n",
        "Python lists => https://www.w3schools.com/python/python_lists.asp (lists are used to store multiple items in a single variable)\n",
        "created train_pfiles list and stored 6 different bearings train data pickle files (bearing1_1_train_data.pkz, bearing1_2_train_data.pkz, \n",
        "bearing2_1_train_data.pkz, bearing2_2_train_data.pkz, bearing3_1_train_data.pkz, bearing3_2_train_data.pkz)\n",
        "\n",
        "created val_pfiles list and stored 6 different bearings validation data pickle files (bearing1_1_val_data.pkz, bearing1_2_val_data.pkz, \n",
        "bearing2_1_val_data.pkz, bearing2_2_val_data.pkz, bearing3_1_val_data.pkz, bearing3_2_val_data.pkz)\n",
        "'''\n",
        "pkzfiles_path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "\n",
        "train_pfiles = [pkzfiles_path+'bearing1_1_train_data.pkz', pkzfiles_path+'bearing1_2_train_data.pkz', \\\n",
        "                pkzfiles_path+'bearing2_1_train_data.pkz', pkzfiles_path+'bearing2_2_train_data.pkz', \\\n",
        "                pkzfiles_path+'bearing3_1_train_data.pkz', pkzfiles_path+'bearing3_2_train_data.pkz']\n",
        "\n",
        "val_pfiles = [pkzfiles_path+'bearing1_1_val_data.pkz', pkzfiles_path+'bearing1_2_val_data.pkz', \\\n",
        "              pkzfiles_path+'bearing2_1_val_data.pkz', pkzfiles_path+'bearing2_2_val_data.pkz', \\\n",
        "              pkzfiles_path+'bearing3_1_val_data.pkz', pkzfiles_path+'bearing3_2_val_data.pkz']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gII_OPIK4zDb"
      },
      "source": [
        "Total length (= total no. data files present in train_dataset and total no. of data files present in val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_EZUt5L26o3",
        "outputId": "c2eb9022-d9c7-491f-f0c0-89aa9d9a1ad7"
      },
      "source": [
        "'''\n",
        "created train_dataset variable and called PHMDataset class and send pfiles list as a parameter inside the class and assigned train_pfiles list to the pfiles.\n",
        "therefore PHMDataset() class now can access to all the train dataset pickle files, hence with train_dataset, can access all train dataset pickle files\n",
        "\n",
        "created val_dataset variable and called PHMDataset class and send pfiles list as a parameter inside the class and assigned val_pfiles list to the pfiles.\n",
        "therefore PHMDataset() class now can access to all the validation dataset pickle files, hence with val_dataset, can access all validation dataset pickle files\n",
        "'''\n",
        "train_dataset = PHMDataset(pfiles=train_pfiles)\n",
        "val_dataset = PHMDataset(pfiles=val_pfiles)\n",
        "print(len(train_dataset), len(val_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6783 751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FGKG0OT5qJ7"
      },
      "source": [
        "'''\n",
        "batch_size => refers to the no. of samples in each batch\n",
        "\n",
        "train_batch_size = 64 => batch size of train dataset is 64, that means train dataset divided into batches (= groups, collection) of data and each batch\n",
        "contains 64 samples (64 datafiles)\n",
        "\n",
        "val_batch_size = 64 => batch size of validation dataset is 64, that means validation dataset divided into batches (= groups, collection) of data and each batch\n",
        "contains 64 samples (64 datafiles)\n",
        "'''\n",
        "train_batch_size = 64\n",
        "val_batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8_MuEIICn3B"
      },
      "source": [
        "torch DataLoader\n",
        "\n",
        "sample dataloader(train_dataloader) that loads batch of 64 samples(data files) at a time from train_dataset (without any multiprocessing (= simultaneously process two or more batches of data)\n",
        "\n",
        "sample dataloader(val_dataloader) that loads batch of 64 samples(data files) at a time from val_dataset (without any multiprocessing (= simultaneously process two or more batches of data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUwgLm7C7kX3"
      },
      "source": [
        "'''\n",
        "DataLoader is an iterator which is used to iterate over train_dataset and val_dataset\n",
        "train_dataset is our train dataset, train_dataset contains all the train data pickle files\n",
        "val_dataset is our validation  dataset, val_dataset contains all the validation data pickle files\n",
        "batch_size => refers to the no. of samples in each batch\n",
        "shuffle => whether we want the data to be shuffled(= rearranged or reorganized) or not. shuffle is a boolean value, either true or false. shuffle=true \n",
        "represents data to be shuffled. shuffle=false represents data not to be shuffled\n",
        "num_workers => no. of processes needed for loading(= activating, starting) the data, here, num_workers=1, therefore 1 time processing will be carried out for\n",
        "data loading \n",
        "'''\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aETq6XAOD4Kf"
      },
      "source": [
        "#Note: \n",
        "\n",
        "pytorch expects (takes) input to a CNN as shape - [NxCxHxW] as opposed to [NxHxWxC] in keras or tensorflow. We already have input image feature shapes in [CxHxW] format stored in pickle files\n",
        "\n",
        "N => batch size, C => no. of channels, H => height of an image, W => width of an image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uumdlO8TK1u"
      },
      "source": [
        "#CNN Model Architecture\n",
        "https://docs.google.com/presentation/d/1zBWkbJVMJlYxCUdSXEP2LSetPUmhVUfumaAzdb5Zd3I/edit?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGGhAoqMEAbu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}